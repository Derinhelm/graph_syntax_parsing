{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Derinhelm/graph_syntax_parsing/blob/main/Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOZsD_SFlTBL"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnKegDLOlSYC",
        "outputId": "3eda8e4d-2c33-4e90-c14b-2a3a2f13c039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvspeC3VaZil"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OShpLwXMo1bx"
      },
      "source": [
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTho6ijao0_w"
      },
      "outputs": [],
      "source": [
        "\n",
        "import logging\n",
        "logger = logging.getLogger('my_logger')\n",
        "\n",
        "# Remove all handlers associated with the root logger object.\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='app.log', # write to this file\n",
        "    filemode='a', # open in append mode\n",
        "    format='%(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "logging.warning('This will get logged to a file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1IM4l2No4-U"
      },
      "outputs": [],
      "source": [
        "logging.warning('New warning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V3mnSrwo6_C"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "print(Path('/content/app.log').read_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yL_DsoclYTr"
      },
      "source": [
        "#uuparser/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhc-Hm49lfaf"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "import os,time\n",
        "from operator import itemgetter\n",
        "import random\n",
        "import json\n",
        "import pathlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "import tqdm\n",
        "\n",
        "\n",
        "UTILS_PATH = pathlib.Path(__file__).parent/\"utils\"\n",
        "\n",
        "\n",
        "class ConllEntry:\n",
        "    def __init__(self, id, form, lemma, pos, cpos, feats=None, parent_id=None, relation=None,\n",
        "        deps=None, misc=None, treebank_id=None, proxy_tbank=None, char_rep=None):\n",
        "\n",
        "        self.id = id\n",
        "        self.form = form\n",
        "        self.char_rep = char_rep if char_rep else form\n",
        "        self.norm = normalize(self.char_rep)\n",
        "        self.cpos = cpos\n",
        "        self.pos = pos\n",
        "        self.parent_id = parent_id\n",
        "        self.relation = relation\n",
        "\n",
        "        self.lemma = lemma\n",
        "        self.feats = feats\n",
        "        self.deps = deps\n",
        "        self.misc = misc\n",
        "\n",
        "        self.pred_parent_id = None\n",
        "        self.pred_relation = None\n",
        "        self.treebank_id = treebank_id\n",
        "        self.proxy_tbank = proxy_tbank\n",
        "\n",
        "        self.pred_pos = None\n",
        "        self.pred_cpos = None\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        values = [str(self.id), self.form, self.lemma, \\\n",
        "                  self.pred_cpos if self.pred_cpos else self.cpos,\\\n",
        "                  self.pred_pos if self.pred_pos else self.pos,\\\n",
        "                  self.feats, str(self.pred_parent_id) if self.pred_parent_id \\\n",
        "                  is not None else str(self.parent_id), self.pred_relation if\\\n",
        "                  self.pred_relation is not None else self.relation, \\\n",
        "                  self.deps, self.misc]\n",
        "        return '\\t'.join(['_' if v is None else v for v in values])\n",
        "\n",
        "class ParseForest:\n",
        "    def __init__(self, sentence):\n",
        "        self.roots = list(sentence)\n",
        "\n",
        "        for root in self.roots:\n",
        "            root.children = []\n",
        "            root.scores = None\n",
        "            root.parent = None\n",
        "            root.pred_parent_id = None\n",
        "            root.pred_relation = None\n",
        "            root.vecs = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.roots)\n",
        "\n",
        "\n",
        "    def Attach(self, parent_index, child_index):\n",
        "        parent = self.roots[parent_index]\n",
        "        child = self.roots[child_index]\n",
        "\n",
        "        child.pred_parent_id = parent.id\n",
        "        del self.roots[child_index]\n",
        "\n",
        "\n",
        "def isProj(sentence):\n",
        "    forest = ParseForest(sentence)\n",
        "    unassigned = {entry.id: sum([1 for pentry in sentence if pentry.parent_id == entry.id]) for entry in sentence}\n",
        "\n",
        "    for _ in xrange(len(sentence)):\n",
        "        for i in xrange(len(forest.roots) - 1):\n",
        "            if forest.roots[i].parent_id == forest.roots[i+1].id and unassigned[forest.roots[i].id] == 0:\n",
        "                unassigned[forest.roots[i+1].id]-=1\n",
        "                forest.Attach(i+1, i)\n",
        "                break\n",
        "            if forest.roots[i+1].parent_id == forest.roots[i].id and unassigned[forest.roots[i+1].id] == 0:\n",
        "                unassigned[forest.roots[i].id]-=1\n",
        "                forest.Attach(i, i+1)\n",
        "                break\n",
        "\n",
        "    return len(forest.roots) == 1\n",
        "\n",
        "\n",
        "def get_irels(data):\n",
        "    \"\"\"\n",
        "    Collect frequencies of words, cpos, pos and deprels + languages.\n",
        "    \"\"\"\n",
        "\n",
        "    # could use sets directly rather than counters for most of these,\n",
        "    # but having the counts might be useful in the future or possibly for debugging etc\n",
        "    relCount = Counter()\n",
        "\n",
        "    for sentence in data:\n",
        "        for node in sentence:\n",
        "            if isinstance(node, ConllEntry):\n",
        "                relCount.update([node.relation])\n",
        "\n",
        "    return list(relCount.keys())\n",
        "\n",
        "\n",
        "def generate_root_token(treebank_id, proxy_tbank):\n",
        "    return ConllEntry(0, '*root*', '*root*', 'ROOT-POS', 'ROOT-CPOS', '_', -1,\n",
        "        'rroot', '_', '_',treebank_id=treebank_id, proxy_tbank=proxy_tbank)\n",
        "\n",
        "\n",
        "def read_conll(filename, treebank_id=None, proxy_tbank=None, maxSize=-1, hard_lim=False, vocab_prep=False, drop_nproj=False, train=True):\n",
        "    # hard lim means capping the corpus size across the whole training procedure\n",
        "    # soft lim means using a sample of the whole corpus at each epoch\n",
        "    fh = open(filename,'r',encoding='utf-8')\n",
        "    logger.info(f\"Reading {filename}\")\n",
        "    if vocab_prep and not hard_lim:\n",
        "        maxSize = -1 # when preparing the vocab with a soft limit we need to use the whole corpus\n",
        "    ts = time.time()\n",
        "    dropped = 0\n",
        "    sents_read = 0\n",
        "    tokens = [generate_root_token(treebank_id, proxy_tbank)]\n",
        "    yield_count = 0\n",
        "    if maxSize > 0 and not hard_lim:\n",
        "        sents = []\n",
        "    for line in fh:\n",
        "        tok = line.strip().split('\\t')\n",
        "        if not tok or line.strip() == '': # empty line, add sentence to list or yield\n",
        "            if len(tokens)>1:\n",
        "                sents_read += 1\n",
        "                conll_tokens = [t for t in tokens if isinstance(t,ConllEntry)]\n",
        "                if not drop_nproj or isProj(conll_tokens): # keep going if it's projective or we're not dropping non-projective sents\n",
        "                    if train:\n",
        "                        inorder_tokens = inorder(conll_tokens)\n",
        "                        for i,t in enumerate(inorder_tokens):\n",
        "                            t.projective_order = i\n",
        "                        for tok in conll_tokens:\n",
        "                            tok.rdeps = [i.id for i in conll_tokens if i.parent_id == tok.id]\n",
        "                            if tok.id != 0:\n",
        "                                tok.parent_entry = [i for i in conll_tokens if i.id == tok.parent_id][0]\n",
        "                    if maxSize > 0:\n",
        "                        if not hard_lim:\n",
        "                            sents.append(tokens)\n",
        "                        else:\n",
        "                            yield tokens\n",
        "                            yield_count += 1\n",
        "                            if yield_count == maxSize:\n",
        "                                logger.info(f\"Capping size of corpus at {yield_count} sentences\")\n",
        "                                break\n",
        "                    else:\n",
        "                        yield tokens\n",
        "                else:\n",
        "                    logger.debug('Non-projective sentence dropped')\n",
        "                    dropped += 1\n",
        "            tokens = [generate_root_token(treebank_id, proxy_tbank)]\n",
        "        else:\n",
        "            if line[0] == '#' or '-' in tok[0] or '.' in tok[0]: # a comment line, add to tokens as is\n",
        "                tokens.append(line.strip())\n",
        "            else: # an actual ConllEntry, add to tokens\n",
        "                char_rep = tok[1] # representation to use in character model\n",
        "                if tok[2] == \"_\":\n",
        "                    tok[2] = tok[1].lower()\n",
        "                token = ConllEntry(int(tok[0]), tok[1], tok[2], tok[4], tok[3], tok[5], int(tok[6]) if tok[6] != '_' else -1, tok[7], tok[8], tok[9],treebank_id=treebank_id,proxy_tbank=proxy_tbank,char_rep=char_rep)\n",
        "\n",
        "                tokens.append(token)\n",
        "\n",
        "    if hard_lim and yield_count < maxSize:\n",
        "        logger.warning(f'Unable to yield {maxSize} sentences, only {yield_count} found')\n",
        "\n",
        "# TODO: deal with case where there are still unyielded tokens\n",
        "# e.g. when there is no newline at end of file\n",
        "#    if len(tokens) > 1:\n",
        "#        yield tokens\n",
        "\n",
        "    logger.debug(f'{sents_read} sentences read')\n",
        "\n",
        "    if maxSize > 0 and not hard_lim:\n",
        "        if len(sents) > maxSize:\n",
        "            sents = random.sample(sents,maxSize)\n",
        "            logger.debug(f\"Yielding {len(sents)} random sentences\")\n",
        "        for toks in sents:\n",
        "            yield toks\n",
        "\n",
        "    te = time.time()\n",
        "    logger.info(f'Time: {te-ts:.2g}s')\n",
        "\n",
        "\n",
        "def write_conll(fn, conll_gen):\n",
        "    logger.info(f\"Writing to {fn}\")\n",
        "    sents = 0\n",
        "    with open(fn, 'w', encoding='utf-8') as fh:\n",
        "        for sentence in conll_gen:\n",
        "            sents += 1\n",
        "            for entry in sentence[1:]:\n",
        "                fh.write(str(entry) + '\\n')\n",
        "            fh.write('\\n')\n",
        "        logger.debug(f\"Wrote {sents} sentences\")\n",
        "\n",
        "\n",
        "numberRegex = re.compile(\"[0-9]+|[0-9]+\\\\.[0-9]+|[0-9]+[0-9,]+\");\n",
        "def normalize(word):\n",
        "    return 'NUM' if numberRegex.match(word) else word.lower()\n",
        "\n",
        "\n",
        "def inorder(sentence):\n",
        "    queue = [sentence[0]]\n",
        "    def inorder_helper(sentence,i):\n",
        "        results = []\n",
        "        left_children = [entry for entry in sentence[:i] if entry.parent_id == i]\n",
        "        for child in left_children:\n",
        "            results += inorder_helper(sentence,child.id)\n",
        "        results.append(sentence[i])\n",
        "\n",
        "        right_children = [entry for entry in sentence[i:] if entry.parent_id == i ]\n",
        "        for child in right_children:\n",
        "            results += inorder_helper(sentence,child.id)\n",
        "        return results\n",
        "    return inorder_helper(sentence,queue[0].id)\n",
        "\n",
        "\n",
        "def set_seeds():\n",
        "    python_seed = 1\n",
        "    logger.debug(\"Using default Python seed\")\n",
        "    random.seed(python_seed)\n",
        "\n",
        "\n",
        "def generate_seed():\n",
        "    return random.randint(0,10**9) # this range seems to work for Dynet and Python's random function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jMvUaVzlnEt"
      },
      "source": [
        "# uuparser/multilayer_perceptron.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqNBhJ3ilnPT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "import torch\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHwx8pKPlyQb"
      },
      "source": [
        "# uuparser/arc_hybrid.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zYFbQ4ivwnL"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from itertools import chain\n",
        "import time, random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import tqdm\n",
        "\n",
        "\n",
        "from jax.numpy import int32\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embed(tokenizer, model, word):\n",
        "\n",
        "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.last_hidden_state[0][0]\n",
        "    return last_hidden_states.detach().cpu()\n",
        "\n",
        "\n",
        "def get_embed_for_sentence(sentence):\n",
        "    word_embeds = torch.empty((len(sentence), 768))\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    for i in range(len(sentence)):\n",
        "        word_embeds[i] = get_embed(tokenizer, model, sentence[i].form)\n",
        "    return word_embeds\n",
        "\n",
        "def create_stack_edges(stack):\n",
        "    if len(stack) == 0:\n",
        "        return torch.stack((torch.tensor([], dtype=torch.int32), torch.tensor([], dtype=torch.int32)), dim=0)\n",
        "    stack_edges = []\n",
        "    if len(stack) == 1:\n",
        "        stack_edges.append((stack[0].id - 1, stack[0].id - 1)) # temporary solution\n",
        "    else:\n",
        "        for i in range(len(stack) - 1): # Represents every two consecutive stack nodes as an edge\n",
        "            stack_edges.append((stack[i].id - 1, stack[i + 1].id - 1))\n",
        "    stack_edges = tuple(zip(*stack_edges))\n",
        "    stack_edges = [torch.tensor(stack_edges[0]), torch.tensor(stack_edges[1])]\n",
        "    return torch.stack(stack_edges, dim=0)\n",
        "\n",
        "def create_buffer_edges(buffer):\n",
        "    if len(buffer) == 0:\n",
        "        return torch.stack((torch.tensor([], dtype=torch.int32), torch.tensor([], dtype=torch.int32)), dim=0)\n",
        "    buffer_edges = []\n",
        "    if len(buffer) == 1:\n",
        "        buffer_edges.append((buffer[0].id - 1, buffer[0].id - 1)) # temporary solution\n",
        "    else:\n",
        "        for i in range(len(buffer) - 1): # Represents every two consecutive buffer nodes as an edge\n",
        "            buffer_edges.append((buffer[i].id - 1, buffer[i + 1].id - 1))\n",
        "    buffer_edges = tuple(zip(*buffer_edges))\n",
        "    buffer_edges = [torch.tensor(buffer_edges[0]), torch.tensor(buffer_edges[1])]\n",
        "    return torch.stack(buffer_edges, dim=0)\n",
        "\n",
        "def create_graph_edges(sentence):\n",
        "    if len(sentence) == 0:\n",
        "        return torch.stack((torch.tensor([], dtype=torch.int32), torch.tensor([], dtype=torch.int32)), dim=0)\n",
        "    graph_edges = []\n",
        "    for node in sentence:\n",
        "        if node.parent_id is not None and node.parent_id != 0:\n",
        "            graph_edges.append((node.parent_id - 1, node.id - 1))\n",
        "    graph_edges = tuple(zip(*graph_edges))\n",
        "    graph_edges = [torch.tensor(graph_edges[0]), torch.tensor(graph_edges[1])]\n",
        "    return torch.stack(graph_edges, dim=0)\n",
        "\n",
        "def config_to_graph(sentence, stack, buffer):\n",
        "    word_embeds = get_embed_for_sentence(sentence)\n",
        "\n",
        "    data = HeteroData()\n",
        "    data['node']['x'] = word_embeds\n",
        "\n",
        "    data[('node', 'graph', 'node')].edge_index = create_graph_edges(sentence)\n",
        "    print(data[('node', 'graph', 'node')].edge_index)\n",
        "    data[('node', 'stack', 'node')].edge_index = create_stack_edges(stack)\n",
        "    data[('node', 'buffer', 'node')].edge_index = create_buffer_edges(buffer)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQsABNd7lnST"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ArcHybridLSTM:\n",
        "    def __init__(self, irels, options):\n",
        "\n",
        "        # import here so we don't load Dynet if just running parser.py --help for example\n",
        "        from uuparser.multilayer_perceptron import MLP\n",
        "\n",
        "        global LEFT_ARC, RIGHT_ARC, SHIFT, SWAP\n",
        "        LEFT_ARC, RIGHT_ARC, SHIFT, SWAP = 0,1,2,3\n",
        "\n",
        "        self.irels = irels\n",
        "\n",
        "        self.activation = options[\"activation\"]\n",
        "\n",
        "        self.hidden_dims = options[\"mlp_hidden_dims\"]\n",
        "\n",
        "        self.mlp_in_dims = 30 # TODO: Create a logical value.\n",
        "\n",
        "        self.metadata = (['node'], [('node', 'graph', 'node'), ('node', 'stack', 'node'), ('node', 'buffer', 'node')])\n",
        "        self.unlabeled_GNN = GNN(hidden_channels=self.hidden_dims, out_channels=4)\n",
        "        self.unlabeled_GNN = to_hetero(self.unlabeled_GNN, self.metadata, aggr='sum')\n",
        "\n",
        "        self.labeled_GNN = GNN(hidden_channels=self.hidden_dims, out_channels=2*len(self.irels)+2)\n",
        "        self.labeled_GNN = to_hetero(self.labeled_GNN, self.metadata, aggr='sum')\n",
        "\n",
        "\n",
        "        self.unlabeled_optimizer = optim.Adam(self.unlabeled_GNN.parameters(), lr=options[\"learning_rate\"]\n",
        "        self.labeled_optimizer = optim.Adam(self.labeled_GNN.parameters(), lr=options[\"learning_rate\"]\n",
        "\n",
        "        self.oracle = options[\"oracle\"]\n",
        "\n",
        "        self.headFlag = options[\"headFlag\"]\n",
        "        self.rlMostFlag = options[\"rlMostFlag\"]\n",
        "        self.rlFlag = options[\"rlFlag\"]\n",
        "        self.k = options[\"k\"]\n",
        "\n",
        "    def __evaluate(self, stack, buf, sentence, train):\n",
        "        \"\"\"\n",
        "        ret = [left arc,\n",
        "               right arc\n",
        "               shift]\n",
        "\n",
        "        RET[i] = (rel, transition, score1, score2) for shift, l_arc and r_arc\n",
        "         shift = 2 (==> rel=None) ; l_arc = 0; r_acr = 1\n",
        "\n",
        "        ret[i][j][2] ~= ret[i][j][3] except the latter is a dynet\n",
        "        expression used in the loss, the first is used in rest of training\n",
        "        \"\"\"\n",
        "\n",
        "        graph = config_to_graph(sentence, stack.roots, buf.roots)\n",
        "        output = self.unlabeled_GNN(graph.x_dict, graph.edge_index_dict)\n",
        "        routput = self.labeled_GNN(graph.x_dict, graph.edge_index_dict)\n",
        "\n",
        "\n",
        "        #scores, unlabeled scores\n",
        "        scrs, uscrs = routput, output\n",
        "\n",
        "        #transition conditions\n",
        "        left_arc_conditions = len(stack) > 0\n",
        "        right_arc_conditions = len(stack) > 1\n",
        "        shift_conditions = buf.roots[0].id != 0\n",
        "        swap_conditions = len(stack) > 0 and stack.roots[-1].id < buf.roots[0].id\n",
        "\n",
        "        if not train:\n",
        "            #(avoiding the multiple roots problem: disallow left-arc from root\n",
        "            #if stack has more than one element\n",
        "            left_arc_conditions = left_arc_conditions and not (buf.roots[0].id == 0 and len(stack) > 1)\n",
        "\n",
        "        uscrs0, uscrs1, uscrs2, uscrs3 = uscrs[0], uscrs[1], uscrs[2], uscrs[3]\n",
        "\n",
        "        if train:\n",
        "            output0, output1, output2, output3 = output[0], output[1], output[2], output[3]\n",
        "\n",
        "\n",
        "            ret = [ [ (rel, LEFT_ARC, scrs[2 + j * 2] + uscrs2, routput[2 + j * 2 ] + output2) for j, rel in enumerate(self.irels) ] if left_arc_conditions else [],\n",
        "                   [ (rel, RIGHT_ARC, scrs[3 + j * 2] + uscrs3, routput[3 + j * 2 ] + output3) for j, rel in enumerate(self.irels) ] if right_arc_conditions else [],\n",
        "                   [ (None, SHIFT, scrs[0] + uscrs0, routput[0] + output0) ] if shift_conditions else [] ,\n",
        "                    [ (None, SWAP, scrs[1] + uscrs1, routput[1] + output1) ] if swap_conditions else [] ]\n",
        "        else:\n",
        "            s1,r1 = max(zip(scrs[2::2],self.irels))\n",
        "            s2,r2 = max(zip(scrs[3::2],self.irels))\n",
        "            s1 = s1 + uscrs2\n",
        "            s2 = s2 + uscrs3\n",
        "            ret = [ [ (r1, LEFT_ARC, s1) ] if left_arc_conditions else [],\n",
        "                   [ (r2, RIGHT_ARC, s2) ] if right_arc_conditions else [],\n",
        "                   [ (None, SHIFT, scrs[0] + uscrs0) ] if shift_conditions else [] ,\n",
        "                    [ (None, SWAP, scrs[1] + uscrs1) ] if swap_conditions else [] ]\n",
        "        return ret\n",
        "\n",
        "    def Load(self, epoch):\n",
        "        unlab_path = 'model_unlab' + '_' + str(epoch)\n",
        "        lab_path = 'model_lab' + '_' + str(epoch)\n",
        "\n",
        "        self.unlabeled_GNN = GNN(hidden_channels=self.hidden_dims, out_channels=4)\n",
        "        self.labeled_GNN = GNN(hidden_channels=self.hidden_dims, out_channels=2*len(self.irels)+2)\n",
        "\n",
        "        unlab_checkpoint = torch.load(unlab_path)\n",
        "        self.unlabeled_GNN.load_state_dict(unlab_checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "        self.unlabeled_GNN = to_hetero(self.unlabeled_GNN, self.metadata, aggr='sum')\n",
        "        self.labeled_GNN = to_hetero(self.labeled_GNN, self.metadata, aggr='sum')\n",
        "\n",
        "        lab_checkpoint = torch.load(lab_path)\n",
        "        self.labeled_GNN.load_state_dict(lab_checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "    def Save(self, epoch):\n",
        "        unlab_path = 'model_unlab' + '_' + str(epoch)\n",
        "        lab_path = 'model_lab' + '_' + str(epoch)\n",
        "        logger.info(f'Saving unlabeled model to {unlab_path}')\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': self.unlabeled_GNN.state_dict()}, unlab_path)\n",
        "        logger.info(f'Saving labeled model to {lab_path}')\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': self.labeled_GNN.state_dict()}, lab_path)\n",
        "\n",
        "    def apply_transition(self,best,stack,buf,hoffset):\n",
        "        if best[1] == SHIFT:\n",
        "            stack.roots.append(buf.roots[0])\n",
        "            del buf.roots[0]\n",
        "\n",
        "        elif best[1] == SWAP:\n",
        "            child = stack.roots.pop()\n",
        "            buf.roots.insert(1,child)\n",
        "\n",
        "        elif best[1] == LEFT_ARC:\n",
        "            child = stack.roots.pop()\n",
        "            parent = buf.roots[0]\n",
        "\n",
        "        elif best[1] == RIGHT_ARC:\n",
        "            child = stack.roots.pop()\n",
        "            parent = stack.roots[-1]\n",
        "\n",
        "        if best[1] == LEFT_ARC or best[1] == RIGHT_ARC:\n",
        "            #attach\n",
        "            child.pred_parent_id = parent.id\n",
        "            child.pred_relation = best[0]\n",
        "\n",
        "    def calculate_cost(self,scores,s0,s1,b,beta,stack_ids):\n",
        "        if len(scores[LEFT_ARC]) == 0:\n",
        "            left_cost = 1\n",
        "        else:\n",
        "            left_cost = len(s0[0].rdeps) + int(s0[0].parent_id != b[0].id and s0[0].id in s0[0].parent_entry.rdeps)\n",
        "\n",
        "\n",
        "        if len(scores[RIGHT_ARC]) == 0:\n",
        "            right_cost = 1\n",
        "        else:\n",
        "            right_cost = len(s0[0].rdeps) + int(s0[0].parent_id != s1[0].id and s0[0].id in s0[0].parent_entry.rdeps)\n",
        "\n",
        "\n",
        "        if len(scores[SHIFT]) == 0:\n",
        "            shift_cost = 1\n",
        "            shift_case = 0\n",
        "        elif len([item for item in beta if item.projective_order < b[0].projective_order and item.id > b[0].id ])> 0:\n",
        "            shift_cost = 0\n",
        "            shift_case = 1\n",
        "        else:\n",
        "            shift_cost = len([d for d in b[0].rdeps if d in stack_ids]) + int(len(s0)>0 and b[0].parent_id in stack_ids[:-1] and b[0].id in b[0].parent_entry.rdeps)\n",
        "            shift_case = 2\n",
        "\n",
        "\n",
        "        if len(scores[SWAP]) == 0 :\n",
        "            swap_cost = 1\n",
        "        elif s0[0].projective_order > b[0].projective_order:\n",
        "            swap_cost = 0\n",
        "            #disable all the others\n",
        "            left_cost = right_cost = shift_cost = 1\n",
        "        else:\n",
        "            swap_cost = 1\n",
        "\n",
        "        costs = (left_cost, right_cost, shift_cost, swap_cost,1)\n",
        "        return costs,shift_case\n",
        "\n",
        "\n",
        "    def oracle_updates(self,best,b,s0,stack_ids,shift_case):\n",
        "        if best[1] == SHIFT:\n",
        "            if shift_case == 2:\n",
        "                if b[0].parent_entry.id in stack_ids[:-1] and b[0].id in b[0].parent_entry.rdeps:\n",
        "                    b[0].parent_entry.rdeps.remove(b[0].id)\n",
        "                blocked_deps = [d for d in b[0].rdeps if d in stack_ids]\n",
        "                for d in blocked_deps:\n",
        "                    b[0].rdeps.remove(d)\n",
        "\n",
        "        elif best[1] == LEFT_ARC or best[1] == RIGHT_ARC:\n",
        "            s0[0].rdeps = []\n",
        "            if s0[0].id in s0[0].parent_entry.rdeps:\n",
        "                s0[0].parent_entry.rdeps.remove(s0[0].id)\n",
        "\n",
        "    def Predict(self, data, datasplit, options):\n",
        "        reached_max_swap = 0\n",
        "\n",
        "        pbar = tqdm.tqdm(\n",
        "            data,\n",
        "            desc=\"Parsing\",\n",
        "            unit=\"sentences\",\n",
        "            mininterval=1.0,\n",
        "            leave=False,\n",
        "            disable=False,\n",
        "        )\n",
        "\n",
        "        for iSentence, osentence in enumerate(pbar,1):\n",
        "            sentence = deepcopy(osentence)\n",
        "            reached_swap_for_i_sentence = False\n",
        "            max_swap = 2*len(sentence)\n",
        "            iSwap = 0\n",
        "            #self.feature_extractor.Init(options)\n",
        "            conll_sentence = [entry for entry in sentence if isinstance(entry, ConllEntry)]\n",
        "            conll_sentence = conll_sentence[1:] + [conll_sentence[0]]\n",
        "            stack = ParseForest([])\n",
        "            buf = ParseForest(conll_sentence)\n",
        "\n",
        "            hoffset = 1 if self.headFlag else 0\n",
        "\n",
        "            for root in conll_sentence:\n",
        "                root.relation = root.relation if root.relation in self.irels else 'runk'\n",
        "\n",
        "\n",
        "            while not (len(buf) == 1 and len(stack) == 0):\n",
        "                scores = self.__evaluate(stack, buf, conll_sentence, False)\n",
        "                best = max(chain(*(scores if iSwap < max_swap else scores[:3] )), key = itemgetter(2) )\n",
        "                if iSwap == max_swap and not reached_swap_for_i_sentence:\n",
        "                    reached_max_swap += 1\n",
        "                    reached_swap_for_i_sentence = True\n",
        "                    logger.debug(f\"reached max swap in {reached_max_swap:d} out of {iSentence:d} sentences\")\n",
        "                self.apply_transition(best,stack,buf,hoffset)\n",
        "                if best[1] == SWAP:\n",
        "                    iSwap += 1\n",
        "\n",
        "            #keep in memory the information we need, not all the vectors\n",
        "            oconll_sentence = [entry for entry in osentence if isinstance(entry, ConllEntry)]\n",
        "            oconll_sentence = oconll_sentence[1:] + [oconll_sentence[0]]\n",
        "            for tok_o, tok in zip(oconll_sentence, conll_sentence):\n",
        "                tok_o.pred_relation = tok.pred_relation\n",
        "                tok_o.pred_parent_id = tok.pred_parent_id\n",
        "            yield osentence\n",
        "\n",
        "    def cost_computing(stack, buf, scores, info):\n",
        "        stack_ids = [sitem.id for sitem in stack.roots]\n",
        "        \n",
        "        s1 = [stack.roots[-2]] if len(stack) > 1 else []\n",
        "        s0 = [stack.roots[-1]] if len(stack) > 0 else []\n",
        "        b = [buf.roots[0]] if len(buf) > 0 else []\n",
        "        beta = buf.roots[1:] if len(buf) > 1 else []\n",
        "\n",
        "        costs, shift_case = self.calculate_cost(scores,s0,s1,b,beta,stack_ids)\n",
        "\n",
        "        bestValid = list(( s for s in chain(*scores) if costs[s[1]] == 0 and ( s[1] == SHIFT or s[1] == SWAP or  s[0] == s0[0].relation ) ))\n",
        "\n",
        "        bestValid = max(bestValid, key=itemgetter(2))\n",
        "        bestWrong = max(( s for s in chain(*scores) if costs[s[1]] != 0 or ( s[1] != SHIFT and s[1] != SWAP and s[0] != s0[0].relation ) ), key=itemgetter(2))\n",
        "        #force swap\n",
        "        if costs[SWAP]== 0:\n",
        "            best = bestValid\n",
        "        else:\n",
        "        #select a transition to follow\n",
        "        # + aggresive exploration\n",
        "        #1: might want to experiment with that parameter\n",
        "            if bestWrong[1] == SWAP:\n",
        "                best = bestValid\n",
        "            else:\n",
        "                best = bestValid if ( (not self.oracle) or (bestValid[2] - bestWrong[2] > 1.0) or (bestValid[2] > bestWrong[2] and random.random() > 0.1) ) else bestWrong\n",
        "\n",
        "        #updates for the dynamic oracle\n",
        "        if self.oracle:\n",
        "            self.oracle_updates(best,b,s0,stack_ids,shift_case)\n",
        "\n",
        "        #labeled errors\n",
        "        if best[1] == LEFT_ARC or best[1] == RIGHT_ARC:\n",
        "            child = s0[0]\n",
        "            if (child.pred_parent_id != child.parent_id or child.pred_relation != child.relation):\n",
        "                info[\"lerrors\"] += 1\n",
        "                #attachment error\n",
        "                if child.pred_parent_id != child.parent_id:\n",
        "                    info[\"eerrors\"] += 1\n",
        "\n",
        "        if bestValid[2] < bestWrong[2] + 1.0:\n",
        "            loss = bestWrong[3] - bestValid[3]\n",
        "            info[\"mloss\"] += 1.0 + bestWrong[2] - bestValid[2]\n",
        "            info[\"eloss\"] += 1.0 + bestWrong[2] - bestValid[2]\n",
        "            info[\"errs\"].append(loss)\n",
        "\n",
        "        #??? when did this happen and why?\n",
        "        if best[1] == 0 or best[1] == 2:\n",
        "            info[\"etotal\"] += 1\n",
        "\n",
        "        return best, info\n",
        "\n",
        "    def train_sentence(sentence, info):\n",
        "            sentence = deepcopy(sentence) # ensures we are working with a clean copy of sentence and allows memory to be recycled each time round the loop\n",
        "\n",
        "            conll_sentence = [entry for entry in sentence if isinstance(entry, ConllEntry)]\n",
        "            conll_sentence = conll_sentence[1:] + [conll_sentence[0]]\n",
        "            stack = ParseForest([])\n",
        "            buf = ParseForest(conll_sentence)\n",
        "            hoffset = 1 if self.headFlag else 0\n",
        "\n",
        "            for root in conll_sentence:\n",
        "                root.relation = root.relation if root.relation in self.irels else 'runk'\n",
        "            \n",
        "            ninf = -float('inf')\n",
        "            while not (len(buf) == 1 and len(stack) == 0):\n",
        "                scores = self.__evaluate(stack, buf, conll_sentence, True)\n",
        "                scores.append([(None, 4, ninf ,None)]) #to ensure that we have at least one wrong operation\n",
        "\n",
        "                best, info = cost_computing(stack, buf, scores, info)\n",
        "\n",
        "                self.apply_transition(best,stack,buf,hoffset)\n",
        "\n",
        "            return info\n",
        "\n",
        "    def error_processing(errs):\n",
        "        self.labeled_optimizer.zero_grad()\n",
        "        self.unlabeled_optimizer.zero_grad()\n",
        "        eerrs = torch.sum(torch.tensor(errs, requires_grad=True))\n",
        "        eerrs.backward()\n",
        "        self.labeled_optimizer.step() # TODO Какой из оптимизаторов ???\n",
        "        self.unlabeled_optimizer.step()\n",
        "        \n",
        "\n",
        "    def Train(self, trainData, options):\n",
        "        info = {}\n",
        "        info[\"mloss\"], info[\"eloss\"], info[\"eerrors\"], info[\"lerrors\"], info[\"etotal\"]  = 0.0, 0.0, 0, 0, 0\n",
        "        info[\"errs\"] = []\n",
        "\n",
        "        beg = time.time()\n",
        "        start = time.time()\n",
        "\n",
        "        random.shuffle(trainData) # in certain cases the data will already have been shuffled after being read from file or while creating dev data\n",
        "        logger.info(f\"Length of training data: {len(trainData)}\")\n",
        "\n",
        "        pbar = tqdm.tqdm(\n",
        "            trainData,\n",
        "            desc=\"Training\",\n",
        "            unit=\"sentences\",\n",
        "            mininterval=1.0,\n",
        "            leave=False,\n",
        "            disable=False,\n",
        "        )\n",
        "\n",
        "        for iSentence, sentence in enumerate(pbar,1):\n",
        "            if iSentence % 100 == 0:\n",
        "                loss_message = (\n",
        "                    f'Processing sentence number: {iSentence}'\n",
        "                    f' Loss: {info[\"eloss\"] / info[\"etotal\"]:.3f}'\n",
        "                    f' Errors: {info[\"eerrors\"] / info[\"etotal\"]:.3f}'\n",
        "                    f' Labeled Errors: {info[\"lerrors\"] / info[\"etotal\"]:.3f}'\n",
        "                    f' Time: {time.time()-start:.3f}s'\n",
        "                )\n",
        "                logger.debug(loss_message)\n",
        "                start = time.time() # TODO: зачем этот параметр ?\n",
        "                info[\"eerrors\"], info[\"eloss\"], info[\"etotal\"], info[\"lerrors\"] = 0, 0.0, 0, 0\n",
        "\n",
        "            info = self.train_sentence(sentence, info)\n",
        "\n",
        "            #footnote 8 in Eli's original paper\n",
        "            if len(info[\"errs\"]) > 50: # or True:\n",
        "                self.error_processing(info[\"errs\"])\n",
        "                info[\"errs\"] = []\n",
        "\n",
        "        if len(info[\"errs\"]) > 0:\n",
        "            self.error_processing(info[\"errs\"])\n",
        "            info[\"errs\"] = []\n",
        "\n",
        "\n",
        "        logger.info(f\"Loss: {info[\"mloss\"]/iSentence}\")\n",
        "        logger.info(f\"Total Training Time: {time.time()-beg:.2g}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01G-nT6mB7H"
      },
      "source": [
        "# uuparser/parser.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "484-h9BXlnU7"
      },
      "outputs": [],
      "source": [
        "import pickle, os, time, sys, copy, itertools, re, random\n",
        "\n",
        "from shutil import copyfile\n",
        "\n",
        "def evaluate_uas(sentence_descr):\n",
        "    #sentence_descr is a list, in which elements 0, 1, 2 are auxiliary\n",
        "    right_parent_tokens = 0\n",
        "    for token in sentence_descr[3:]:\n",
        "        if isinstance(token, ConllEntry): # TODO: изучить случаи, когда не ConllEntry - ошибка считывания?\n",
        "          if token.pred_parent_id == token.parent_id:\n",
        "              right_parent_tokens += 1\n",
        "        #print(\"pred_parent:\", token.pred_parent_id, \"real_parent:\", token.parent_id)\n",
        "    uas = right_parent_tokens / (len(sentence_descr) - 3)\n",
        "    return uas\n",
        "\n",
        "def evaluate_uas_epoche(sentence_list):\n",
        "    summ_uas = 0\n",
        "    for sent in sentence_list:\n",
        "        summ_uas += evaluate_uas(sent)\n",
        "    return summ_uas / len(sentence_list)\n",
        "\n",
        "def run(traindata, valdata, testdata, options):\n",
        "\n",
        "    from uuparser.arc_hybrid import ArcHybridLSTM\n",
        "    #logger.info('Working with a transition-based parser')\n",
        "\n",
        "    irels = get_irels(traindata)\n",
        "    logger.debug('Initializing the model')\n",
        "    parser = ArcHybridLSTM(irels, options)\n",
        "\n",
        "    dev_best = [options[\"epochs\"],-1.0] # best epoch, best score\n",
        "\n",
        "    for epoch in range(options[\"first_epoch\"], options[\"epochs\"] + 1):\n",
        "        # Training\n",
        "        logger.info(f'Starting epoch {epoch} (training)')\n",
        "        parser.Train(traindata,options)\n",
        "        logger.info(f'Finished epoch {epoch} (training)')\n",
        "\n",
        "        parser.Save(epoch)\n",
        "\n",
        "        logger.info(f\"Predicting on dev data\")\n",
        "        dev_pred = list(parser.Predict(valdata,\"dev\",options))\n",
        "        mean_dev_score = evaluate_uas_epoche(dev_pred)\n",
        "        logger.info(f\"Dev score {mean_dev_score:.2f} at epoch {epoch:d}\")\n",
        "        print(f\"Dev score {mean_dev_score:.2f} at epoch {epoch:d}\")\n",
        "\n",
        "        if mean_dev_score > dev_best[1]:\n",
        "            dev_best = [epoch,mean_dev_score] # update best dev score\n",
        "\n",
        "    logger.info(f\"Loading best model from epoche{dev_best[0]:d}\")\n",
        "    # Loading best_models to parser.labeled_GNN and parser.unlabeled_GNN\n",
        "    parser.Load(epoch)\n",
        "\n",
        "    logger.info(f\"Predicting on test data\")\n",
        "\n",
        "    test_pred = list(parser.Predict(testdata,\"test\",options))\n",
        "    mean_test_score = evaluate_uas_epoche(test_pred)\n",
        "\n",
        "    logger.info(f\"On test obtained UAS score of {mean_test_score:.2f}\")\n",
        "    print(f\"On test obtained UAS score of {mean_test_score:.2f}\")\n",
        "\n",
        "\n",
        "    logger.debug('Finished predicting')\n",
        "\n",
        "\n",
        "def main():\n",
        "    options = {}\n",
        "    options[\"activation\"] = \"tanh\" # Activation function in the MLP\n",
        "    options[\"mlp_hidden_dims\"] = 100 # MLP hidden layer dimensions\n",
        "    options[\"learning_rate\"] = 0.001 # Learning rate for neural network optimizer\n",
        "    options[\"oracle\"] = True # Use the static oracle instead of the dynamic oracle\n",
        "    options[\"headFlag\"] = True # Disable using the head of word vectors fed to the MLP\n",
        "    options[\"rlMostFlag\"] = True # Disable using leftmost and rightmost dependents of words fed to the MLP\n",
        "    options[\"rlFlag\"] = False\n",
        "    options[\"k\"] = 3 # Number of stack elements to feed to MLP\n",
        "    options[\"epochs\"] = 30 # Number of epochs\n",
        "    options[\"first_epoch\"] = 1\n",
        "    options[\"max_sentences\"] = -1 # Only train using n sentences per epoch\n",
        "\n",
        "    # really important to do this before anything else to make experiments reproducible\n",
        "    set_seeds()\n",
        "\n",
        "    train_dir = 'sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu'\n",
        "    val_dir = 'sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-val.conllu'\n",
        "    test_dir = 'sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu'\n",
        "\n",
        "    train = list(read_conll(train_dir, maxSize=options[\"max_sentences\"]))\n",
        "    val = list(read_conll(val_dir, maxSize=options[\"max_sentences\"]))\n",
        "    test = list(read_conll(test_dir, maxSize=options[\"max_sentences\"]))\n",
        "    run(train, val, test, options)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcaLj-fjlnXw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOT3NenHSk/GyOGaazOj9eJ",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
