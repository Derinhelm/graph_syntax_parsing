{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Derinhelm/graph_syntax_parsing/blob/main/Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOZsD_SFlTBL"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnKegDLOlSYC",
        "outputId": "11332f47-1656-41ae-be19-e968669224a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pvspeC3VaZil"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OShpLwXMo1bx"
      },
      "source": [
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "kTho6ijao0_w"
      },
      "outputs": [],
      "source": [
        "\n",
        "import logging\n",
        "logger = logging.getLogger('my_logger')\n",
        "\n",
        "# Remove all handlers associated with the root logger object.\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='app.log', # write to this file\n",
        "    filemode='a', # open in append mode\n",
        "    format='%(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "logging.getLogger().setLevel(logging.DEBUG)\n",
        "\n",
        "logging.getLogger(\"urllib3.connectionpool\").disabled = True\n",
        "logging.getLogger(\"filelock\").disabled = True\n",
        "\n",
        "logging.warning('This will get logged to a file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "h1IM4l2No4-U"
      },
      "outputs": [],
      "source": [
        "logging.warning('New warning')\n",
        "logging.debug('New debug')\n",
        "logging.info('New info')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V3mnSrwo6_C",
        "outputId": "f2ef36a4-c718-4d68-f387-17fd19cf2823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root - WARNING - This will get logged to a file\n",
            "root - WARNING - New warning\n",
            "root - DEBUG - New debug\n",
            "root - INFO - New info\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "print(Path('/content/app.log').read_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yL_DsoclYTr"
      },
      "source": [
        "#uuparser/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Uhc-Hm49lfaf"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "import os,time\n",
        "from operator import itemgetter\n",
        "import random\n",
        "import json\n",
        "import pathlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class ConllEntry:\n",
        "    def __init__(self, id, form, lemma, pos, cpos, feats=None, parent_id=None, relation=None,\n",
        "        deps=None, misc=None):\n",
        "\n",
        "        self.id = id\n",
        "        self.form = form\n",
        "        self.cpos = cpos\n",
        "        self.pos = pos\n",
        "        self.parent_id = parent_id\n",
        "        self.relation = relation\n",
        "\n",
        "        self.lemma = lemma\n",
        "        self.feats = feats\n",
        "        self.deps = deps\n",
        "        self.misc = misc\n",
        "\n",
        "        self.pred_parent_id = None\n",
        "        self.pred_relation = None\n",
        "\n",
        "        self.pred_pos = None\n",
        "        self.pred_cpos = None\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        '''values = [str(self.id), self.form, self.lemma, \\\n",
        "                  self.pred_cpos if self.pred_cpos else self.cpos,\\\n",
        "                  self.pred_pos if self.pred_pos else self.pos,\\\n",
        "                  self.feats, str(self.pred_parent_id) if self.pred_parent_id \\\n",
        "                  is not None else str(self.parent_id), self.pred_relation if\\\n",
        "                  self.pred_relation is not None else self.relation, \\\n",
        "                  self.deps, self.misc]\n",
        "        return '\\t'.join(['_' if v is None else v for v in values])'''\n",
        "        return self.form + \" \" + str(self.id)\n",
        "\n",
        "class ParseForest:\n",
        "    def __init__(self, sentence):\n",
        "        self.roots = list(sentence)\n",
        "\n",
        "        for root in self.roots:\n",
        "            root.children = []\n",
        "            root.scores = None\n",
        "            root.parent = None\n",
        "            root.pred_parent_id = None\n",
        "            root.pred_relation = None\n",
        "            root.vecs = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.roots)\n",
        "\n",
        "\n",
        "    def Attach(self, parent_index, child_index):\n",
        "        parent = self.roots[parent_index]\n",
        "        child = self.roots[child_index]\n",
        "\n",
        "        child.pred_parent_id = parent.id\n",
        "        del self.roots[child_index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \" \".join(map(str, self.roots))\n",
        "\n",
        "\n",
        "def isProj(sentence):\n",
        "    forest = ParseForest(sentence)\n",
        "    unassigned = {entry.id: sum([1 for pentry in sentence if pentry.parent_id == entry.id]) for entry in sentence}\n",
        "\n",
        "    for _ in xrange(len(sentence)):\n",
        "        for i in xrange(len(forest.roots) - 1):\n",
        "            if forest.roots[i].parent_id == forest.roots[i+1].id and unassigned[forest.roots[i].id] == 0:\n",
        "                unassigned[forest.roots[i+1].id]-=1\n",
        "                forest.Attach(i+1, i)\n",
        "                break\n",
        "            if forest.roots[i+1].parent_id == forest.roots[i].id and unassigned[forest.roots[i+1].id] == 0:\n",
        "                unassigned[forest.roots[i].id]-=1\n",
        "                forest.Attach(i, i+1)\n",
        "                break\n",
        "\n",
        "    return len(forest.roots) == 1\n",
        "\n",
        "\n",
        "def get_irels(data):\n",
        "    \"\"\"\n",
        "    Collect frequencies of words, cpos, pos and deprels + languages.\n",
        "    \"\"\"\n",
        "\n",
        "    # could use sets directly rather than counters for most of these,\n",
        "    # but having the counts might be useful in the future or possibly for debugging etc\n",
        "    relCount = Counter()\n",
        "\n",
        "    for sentence in data:\n",
        "        for node in sentence:\n",
        "            if isinstance(node, ConllEntry):\n",
        "                relCount.update([node.relation])\n",
        "\n",
        "    return list(relCount.keys())\n",
        "\n",
        "\n",
        "def generate_root_token():\n",
        "    return ConllEntry(0, '*root*', '*root*', 'ROOT-POS', 'ROOT-CPOS', '_', -1,\n",
        "        'rroot', '_', '_')\n",
        "\n",
        "\n",
        "def read_conll(filename, drop_nproj=False, train=True):\n",
        "    fh = open(filename,'r',encoding='utf-8')\n",
        "    logging.info(f\"Reading {filename}\")\n",
        "    ts = time.time()\n",
        "    dropped = 0\n",
        "    sents_read = 0\n",
        "    sentences = []\n",
        "    tokens = [generate_root_token()]\n",
        "    words = [] # all words from the dataset\n",
        "    for line in fh:\n",
        "        tok = line.strip().split('\\t')\n",
        "        if not tok or line.strip() == '': # empty line, add sentence to list or yield\n",
        "            if len(tokens) > 1:\n",
        "                sents_read += 1\n",
        "                conll_tokens = [t for t in tokens if isinstance(t,ConllEntry)]\n",
        "                if not drop_nproj or isProj(conll_tokens): \n",
        "                    # keep going if it's projective or we're not dropping non-projective sents\n",
        "                    if train:\n",
        "                        inorder_tokens = inorder(conll_tokens)\n",
        "                        for i,t in enumerate(inorder_tokens):\n",
        "                            t.projective_order = i\n",
        "                        for tok in conll_tokens:\n",
        "                            tok.rdeps = [i.id for i in conll_tokens if i.parent_id == tok.id]\n",
        "                            if tok.id != 0:\n",
        "                                tok.parent_entry = [i for i in conll_tokens if i.id == tok.parent_id][0]\n",
        "                    sentences.append(tokens)\n",
        "                else:\n",
        "                    logging.debug('Non-projective sentence dropped')\n",
        "                    dropped += 1\n",
        "            tokens = [generate_root_token()]\n",
        "        else:\n",
        "            if line[0] == '#' or '-' in tok[0] or '.' in tok[0]: # a comment line, add to tokens as is\n",
        "                tokens.append(line.strip())\n",
        "            else: # an actual ConllEntry, add to tokens\n",
        "                if tok[2] == \"_\":\n",
        "                    tok[2] = tok[1].lower()\n",
        "                lemma = tok[2]\n",
        "                words.append(lemma)\n",
        "                token = ConllEntry(int(tok[0]), tok[1], lemma, tok[4], tok[3], tok[5], \\\n",
        "                    int(tok[6]) if tok[6] != '_' else -1, tok[7], tok[8], tok[9])\n",
        "\n",
        "                tokens.append(token)\n",
        "\n",
        "# deal with case where there are still tokens, that aren`t in sentences list\n",
        "# e.g. when there is no newline at end of file\n",
        "    if len(tokens) > 1:\n",
        "        sentences.append(tokens)\n",
        "\n",
        "    logging.debug(f'{sents_read} sentences read')\n",
        "\n",
        "    te = time.time()\n",
        "    logging.info(f'Time: {te-ts:.2g}s')\n",
        "    return sentences, words\n",
        "\n",
        "\n",
        "def write_conll(fn, conll_gen):\n",
        "    logging.info(f\"Writing to {fn}\")\n",
        "    sents = 0\n",
        "    with open(fn, 'w', encoding='utf-8') as fh:\n",
        "        for sentence in conll_gen:\n",
        "            sents += 1\n",
        "            for entry in sentence[1:]:\n",
        "                fh.write(str(entry) + '\\n')\n",
        "            fh.write('\\n')\n",
        "        logging.debug(f\"Wrote {sents} sentences\")\n",
        "\n",
        "\n",
        "numberRegex = re.compile(\"[0-9]+|[0-9]+\\\\.[0-9]+|[0-9]+[0-9,]+\");\n",
        "def normalize(word):\n",
        "    return 'NUM' if numberRegex.match(word) else word.lower()\n",
        "\n",
        "\n",
        "def inorder(sentence):\n",
        "    queue = [sentence[0]]\n",
        "    def inorder_helper(sentence,i):\n",
        "        results = []\n",
        "        left_children = [entry for entry in sentence[:i] if entry.parent_id == i]\n",
        "        for child in left_children:\n",
        "            results += inorder_helper(sentence,child.id)\n",
        "        results.append(sentence[i])\n",
        "\n",
        "        right_children = [entry for entry in sentence[i:] if entry.parent_id == i ]\n",
        "        for child in right_children:\n",
        "            results += inorder_helper(sentence,child.id)\n",
        "        return results\n",
        "    return inorder_helper(sentence,queue[0].id)\n",
        "\n",
        "\n",
        "def set_seeds():\n",
        "    python_seed = 1\n",
        "    logging.debug(\"Using default Python seed\")\n",
        "    random.seed(python_seed)\n",
        "\n",
        "\n",
        "def generate_seed():\n",
        "    return random.randint(0,10**9) # this range seems to work for Dynet and Python's random function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jMvUaVzlnEt"
      },
      "source": [
        "# uuparser/multilayer_perceptron.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bqNBhJ3ilnPT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "import torch\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHwx8pKPlyQb"
      },
      "source": [
        "# uuparser/arc_hybrid.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "3zYFbQ4ivwnL"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from itertools import chain\n",
        "import time, random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import tqdm\n",
        "\n",
        "\n",
        "from jax.numpy import int32\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeMXyAEjgagf"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "b0hMZDgY7MC_"
      },
      "outputs": [],
      "source": [
        "def create_stack_edges(stack):\n",
        "    if len(stack) == 0:\n",
        "        return torch.stack((torch.tensor([], dtype=torch.int32), torch.tensor([], dtype=torch.int32)), dim=0)\n",
        "    stack_edges = []\n",
        "    if len(stack) == 1:\n",
        "        stack_edges.append((stack[0].id - 1, stack[0].id - 1)) # temporary solution\n",
        "    else:\n",
        "        for i in range(len(stack) - 1): # Represents every two consecutive stack nodes as an edge\n",
        "            stack_edges.append((stack[i].id - 1, stack[i + 1].id - 1))\n",
        "    stack_edges = tuple(zip(*stack_edges))\n",
        "    stack_edges = [torch.tensor(stack_edges[0]), torch.tensor(stack_edges[1])]\n",
        "    return torch.stack(stack_edges, dim=0)\n",
        "\n",
        "def create_buffer_edges(buffer):\n",
        "    if len(buffer) == 0 or len(buffer) == 1: # Last element is a technical root element.\n",
        "        return torch.stack((torch.tensor([], dtype=torch.int32), torch.tensor([], dtype=torch.int32)), dim=0)\n",
        "    buffer_edges = []\n",
        "    if len(buffer) == 2: # Last element is a technical root element.\n",
        "        buffer_edges.append((buffer[0].id - 1, buffer[0].id - 1)) # temporary solution\n",
        "    else:\n",
        "        for i in range(len(buffer) - 2): # Last element is a technical root element.\n",
        "        # Represents every two consecutive buffer nodes as an edge\n",
        "            buffer_edges.append((buffer[i].id - 1, buffer[i + 1].id - 1))\n",
        "    buffer_edges = tuple(zip(*buffer_edges))\n",
        "    buffer_edges = [torch.tensor(buffer_edges[0]), torch.tensor(buffer_edges[1])]\n",
        "    return torch.stack(buffer_edges, dim=0)\n",
        "\n",
        "def create_graph_edges(sentence):\n",
        "    graph_edges = []\n",
        "    for node in sentence:\n",
        "        if node.pred_parent_id is not None and node.pred_parent_id != 0 and node.pred_parent_id != -1:\n",
        "            graph_edges.append((node.pred_parent_id - 1, node.id - 1))\n",
        "    if len(graph_edges) == 0:\n",
        "        return torch.stack((torch.tensor([], dtype=torch.int32), torch.tensor([], dtype=torch.int32)), dim=0)\n",
        "    graph_edges = tuple(zip(*graph_edges))\n",
        "    graph_edges = [torch.tensor(graph_edges[0]), torch.tensor(graph_edges[1])]\n",
        "    return torch.stack(graph_edges, dim=0)\n",
        "\n",
        "class Configuration:\n",
        "    def __init__(self, sentence, irels):\n",
        "        self.sentence = deepcopy(sentence) \n",
        "        # ensures we are working with a clean copy of sentence and allows memory to be recycled each time round the loop\n",
        "        self.sentence = [entry for entry in self.sentence if isinstance(entry, ConllEntry)]\n",
        "        self.sentence = self.sentence[1:] + [self.sentence[0]]\n",
        "        self.stack = ParseForest([])\n",
        "        self.buffer = ParseForest(self.sentence)\n",
        "        for root in self.sentence:\n",
        "            root.relation = root.relation if root.relation in irels else 'runk'\n",
        "\n",
        "\n",
        "    def config_to_graph(self, embeds):\n",
        "        word_embeds = torch.empty((len(self.sentence), 768))\n",
        "        for i in range(len(self.sentence) - 1): # Last element is a technical root element.\n",
        "            word_embeds[i] = embeds[self.sentence[i].lemma]\n",
        "\n",
        "        data = HeteroData()\n",
        "        data['node']['x'] = word_embeds\n",
        "\n",
        "        data[('node', 'graph', 'node')].edge_index = create_graph_edges(self.sentence)\n",
        "        data[('node', 'stack', 'node')].edge_index = create_stack_edges(self.stack)\n",
        "        data[('node', 'buffer', 'node')].edge_index = create_buffer_edges(self.buffer)\n",
        "        return data\n",
        "\n",
        "    def apply_transition(self, best):\n",
        "        if best[1] == SHIFT:\n",
        "            self.stack.roots.append(self.buffer.roots[0])\n",
        "            del self.buffer.roots[0]\n",
        "\n",
        "        elif best[1] == SWAP:\n",
        "            child = self.stack.roots.pop()\n",
        "            self.buffer.roots.insert(1,child)\n",
        "\n",
        "        elif best[1] == LEFT_ARC:\n",
        "            child = self.stack.roots.pop()\n",
        "            parent = self.buffer.roots[0]\n",
        "\n",
        "        elif best[1] == RIGHT_ARC:\n",
        "            child = self.stack.roots.pop()\n",
        "            parent = self.stack.roots[-1]\n",
        "\n",
        "        if best[1] == LEFT_ARC or best[1] == RIGHT_ARC:\n",
        "            #attach\n",
        "            child.pred_parent_id = parent.id\n",
        "            child.pred_relation = best[0]\n",
        "\n",
        "    def get_stack_ids(self):\n",
        "        return [sitem.id for sitem in self.stack.roots]\n",
        "\n",
        "    def get_stack_last_element(self):\n",
        "        return [self.stack.roots[-1]] if len(self.stack) > 0 else [] # Last stack element\n",
        "\n",
        "    def get_stack_penultimate_element(self):\n",
        "        return [self.stack.roots[-2]] if len(self.stack) > 1 else [] # Penultimate stack element\n",
        "\n",
        "    def get_buffer_head(self):\n",
        "        return [self.buffer.roots[0]] if len(self.buffer) > 0 else [] # Head buffer element\n",
        "\n",
        "    def get_buffer_tail(self):\n",
        "        return self.buffer.roots[1:] if len(self.buffer) > 1 else [] # Tail of buffer\n",
        "\n",
        "    def get_sentence(self):\n",
        "        return self.sentence\n",
        "\n",
        "    def is_end(self):\n",
        "        return len(self.buffer) == 1 and len(self.stack) == 0\n",
        "\n",
        "    def check_left_arc_conditions(self):\n",
        "        return len(self.stack) > 0\n",
        "    \n",
        "    def check_not_train_left_arc_conditions(self):\n",
        "            #(avoiding the multiple roots problem: disallow left-arc from root\n",
        "            #if stack has more than one element\n",
        "        return self.left_arc_conditions() and not (self.buffer.roots[0].id == 0 and len(self.stack) > 1)\n",
        "\n",
        "    def check_right_arc_conditions(self):\n",
        "        return len(self.stack) > 1  \n",
        "\n",
        "    def check_shift_conditions(self):\n",
        "        return self.buffer.roots[0].id != 0\n",
        "\n",
        "    def check_swap_conditions(self):\n",
        "        return len(self.stack) > 0 and self.stack.roots[-1].id < self.buffer.roots[0].id\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"stack:\" + str(self.stack) + \"\\n\" + \"buffer:\" + str(self.buffer) + \"\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxiPi0Trgh1_"
      },
      "source": [
        "## Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "eQsABNd7lnST"
      },
      "outputs": [],
      "source": [
        "class Parser:\n",
        "    def __init__(self, options, out_irels_dims):\n",
        "\n",
        "        global LEFT_ARC, RIGHT_ARC, SHIFT, SWAP\n",
        "        LEFT_ARC, RIGHT_ARC, SHIFT, SWAP = 0,1,2,3\n",
        "\n",
        "        self.oracle = options[\"oracle\"]\n",
        "\n",
        "        self.hidden_dims = options[\"hidden_dims\"]\n",
        "        self.out_irels_dims = out_irels_dims\n",
        "\n",
        "        self.metadata = (['node'], [('node', 'graph', 'node'), ('node', 'stack', 'node'), ('node', 'buffer', 'node')])\n",
        "        self.unlabeled_GNN = GNN(hidden_channels=self.hidden_dims, out_channels=4)\n",
        "        self.unlabeled_GNN = to_hetero(self.unlabeled_GNN, self.metadata, aggr='sum')\n",
        "\n",
        "        self.labeled_GNN = GNN(hidden_channels=self.hidden_dims, out_channels=2*self.out_irels_dims+2)\n",
        "        self.labeled_GNN = to_hetero(self.labeled_GNN, self.metadata, aggr='sum')\n",
        "\n",
        "        self.unlabeled_optimizer = optim.Adam(self.unlabeled_GNN.parameters(), lr=options[\"learning_rate\"])\n",
        "        self.labeled_optimizer = optim.Adam(self.labeled_GNN.parameters(), lr=options[\"learning_rate\"])\n",
        "\n",
        "    def Load(self, epoch):\n",
        "        unlab_path = 'model_unlab' + '_' + str(epoch)\n",
        "        lab_path = 'model_lab' + '_' + str(epoch)\n",
        "\n",
        "        self.unlabeled_GNN = GNN(hidden_channels=self.hidden_dims, out_channels=4)\n",
        "        self.labeled_GNN = GNN(hidden_channels=self.hidden_dims, out_channels=2*self.out_irels_dims+2)\n",
        "\n",
        "        unlab_checkpoint = torch.load(unlab_path)\n",
        "        self.unlabeled_GNN.load_state_dict(unlab_checkpoint['model_state_dict'], strict=False)\n",
        "\n",
        "\n",
        "        self.unlabeled_GNN = to_hetero(self.unlabeled_GNN, self.metadata, aggr='sum')\n",
        "        self.labeled_GNN = to_hetero(self.labeled_GNN, self.metadata, aggr='sum')\n",
        "\n",
        "        lab_checkpoint = torch.load(lab_path)\n",
        "        self.labeled_GNN.load_state_dict(lab_checkpoint['model_state_dict'], strict=False)\n",
        "\n",
        "\n",
        "    def Save(self, epoch):\n",
        "        unlab_path = 'model_unlab' + '_' + str(epoch)\n",
        "        lab_path = 'model_lab' + '_' + str(epoch)\n",
        "        logging.info(f'Saving unlabeled model to {unlab_path}')\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': self.unlabeled_GNN.state_dict()}, unlab_path)\n",
        "        logging.info(f'Saving labeled model to {lab_path}')\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': self.labeled_GNN.state_dict()}, lab_path)\n",
        "\n",
        "    def train_evaluate(self, config, scrs, uscrs, routput, output, irels):\n",
        "                #transition conditions\n",
        "        left_arc_conditions = config.check_left_arc_conditions()\n",
        "        right_arc_conditions = config.check_right_arc_conditions()\n",
        "        shift_conditions = config.check_shift_conditions()\n",
        "        swap_conditions = config.check_swap_conditions()\n",
        "\n",
        "        ret = [ [ (rel, LEFT_ARC, scrs[2 + j * 2] + uscrs[2], routput[2 + j * 2 ] + output[2]) \\\n",
        "                    for j, rel in enumerate(irels) ] if left_arc_conditions else [],\n",
        "                [ (rel, RIGHT_ARC, scrs[3 + j * 2] + uscrs[3], routput[3 + j * 2 ] + output[3]) \\\n",
        "                    for j, rel in enumerate(irels) ] if right_arc_conditions else [],\n",
        "                [ (None, SHIFT, scrs[0] + uscrs[0], routput[0] + output[0]) ] if shift_conditions else [] ,\n",
        "                [ (None, SWAP, scrs[1] + uscrs[1], routput[1] + output[1]) ] if swap_conditions else [] ]\n",
        "        return ret\n",
        "\n",
        "    def test_evaluate(self, config, scrs, uscrs, irels):\n",
        "        \n",
        "        #transition conditions\n",
        "        right_arc_conditions = config.check_right_arc_conditions()\n",
        "        shift_conditions = config.check_shift_conditions()\n",
        "        swap_conditions = config.check_swap_conditions()\n",
        "\n",
        "        #(avoiding the multiple roots problem: disallow left-arc from root\n",
        "        #if stack has more than one element\n",
        "        left_arc_conditions = config.check_not_train_left_arc_conditions()\n",
        "\n",
        "        s1,r1 = max(zip(scrs[2::2],irels))\n",
        "        s2,r2 = max(zip(scrs[3::2],irels))\n",
        "        s1 = s1 + uscrs[2]\n",
        "        s2 = s2 + uscrs[3]\n",
        "        ret = [ [ (r1, LEFT_ARC, s1) ] if left_arc_conditions else [],\n",
        "                [ (r2, RIGHT_ARC, s2) ] if right_arc_conditions else [],\n",
        "                [ (None, SHIFT, scrs[0] + uscrs[0]) ] if shift_conditions else [] ,\n",
        "                [ (None, SWAP, scrs[1] + uscrs[1]) ] if swap_conditions else [] ]\n",
        "        return ret\n",
        "    \n",
        "\n",
        "    def __evaluate(self, config, train, irels, embeds):\n",
        "        \"\"\"\n",
        "        ret = [left arc,\n",
        "               right arc\n",
        "               shift]\n",
        "\n",
        "        RET[i] = (rel, transition, score1, score2) for shift, l_arc and r_arc\n",
        "         shift = 2 (==> rel=None) ; l_arc = 0; r_acr = 1\n",
        "\n",
        "        ret[i][j][2] ~= ret[i][j][3] except the latter is a dynet\n",
        "        expression used in the loss, the first is used in rest of training\n",
        "        \"\"\"\n",
        "\n",
        "        graph = config.config_to_graph(embeds)\n",
        "        output = self.unlabeled_GNN(graph.x_dict, graph.edge_index_dict)\n",
        "        output = torch.sum(output['node'], dim=0)\n",
        "        routput = self.labeled_GNN(graph.x_dict, graph.edge_index_dict)\n",
        "        routput = torch.sum(routput['node'], dim=0)\n",
        "\n",
        "        #scores, unlabeled scores\n",
        "        scrs, uscrs = routput, output # TODO: В dynet здесь делают value(). \n",
        "        #Для распространения ошибки?? используется 3 параметр, вычисляемый на routput/output, для остального - вычисленное на scrs/uscrs\n",
        "\n",
        "        if train:\n",
        "            return self.train_evaluate(config, scrs, uscrs, routput, output, irels)\n",
        "        else:\n",
        "            return self.test_evaluate(config, scrs, uscrs, irels)\n",
        "\n",
        "    def Predict(self, data, datasplit, options, irels, embeds):\n",
        "        reached_max_swap = 0\n",
        "\n",
        "        pbar = tqdm.tqdm(\n",
        "            data,\n",
        "            desc=\"Parsing\",\n",
        "            unit=\"sentences\",\n",
        "            mininterval=1.0,\n",
        "            leave=False,\n",
        "            disable=False,\n",
        "        )\n",
        "\n",
        "        for iSentence, osentence in enumerate(pbar,1):\n",
        "            config = Configuration(osentence, irels)\n",
        "            max_swap = 2*len(osentence)\n",
        "            reached_swap_for_i_sentence = False\n",
        "            iSwap = 0\n",
        "\n",
        "            while not config.is_end():\n",
        "                scores = self.__evaluate(config, False, irels, embeds)\n",
        "                best = max(chain(*(scores if iSwap < max_swap else scores[:3] )), key = itemgetter(2) )\n",
        "                if iSwap == max_swap and not reached_swap_for_i_sentence:\n",
        "                    reached_max_swap += 1\n",
        "                    reached_swap_for_i_sentence = True\n",
        "                    logging.debug(f\"reached max swap in {reached_max_swap:d} out of {iSentence:d} sentences\")\n",
        "                config.apply_transition(best)\n",
        "                if best[1] == SWAP:\n",
        "                    iSwap += 1\n",
        "\n",
        "            #keep in memory the information we need, not all the vectors\n",
        "            oconll_sentence = [entry for entry in osentence if isinstance(entry, ConllEntry)]\n",
        "            oconll_sentence = oconll_sentence[1:] + [oconll_sentence[0]]\n",
        "            conll_sentence = config.get_sentence()\n",
        "            for tok_o, tok in zip(oconll_sentence, conll_sentence):\n",
        "                tok_o.pred_relation = tok.pred_relation\n",
        "                tok_o.pred_parent_id = tok.pred_parent_id\n",
        "            yield osentence\n",
        "\n",
        "    def calculate_cost(self, config, scores):\n",
        "        s1 = config.get_stack_penultimate_element() # Penultimate stack element\n",
        "        s0 = config.get_stack_last_element() # Last stack element\n",
        "        b = config.get_buffer_head() # Head buffer element\n",
        "        beta = config.get_buffer_tail() # Tail (list) of buffer\n",
        "\n",
        "        stack_ids = config.get_stack_ids()\n",
        "\n",
        "        if len(scores[LEFT_ARC]) == 0:\n",
        "            left_cost = 1\n",
        "        else:\n",
        "            left_cost = len(s0[0].rdeps) + int(s0[0].parent_id != b[0].id and s0[0].id in s0[0].parent_entry.rdeps)\n",
        "\n",
        "\n",
        "        if len(scores[RIGHT_ARC]) == 0:\n",
        "            right_cost = 1\n",
        "        else:\n",
        "            right_cost = len(s0[0].rdeps) + int(s0[0].parent_id != s1[0].id and s0[0].id in s0[0].parent_entry.rdeps)\n",
        "\n",
        "\n",
        "        if len(scores[SHIFT]) == 0:\n",
        "            shift_cost = 1\n",
        "            shift_case = 0\n",
        "        elif len([item for item in beta if item.projective_order < b[0].projective_order and item.id > b[0].id ])> 0:\n",
        "            shift_cost = 0\n",
        "            shift_case = 1\n",
        "        else:\n",
        "            shift_cost = len([d for d in b[0].rdeps if d in stack_ids]) + \\\n",
        "                int(len(s0)>0 and b[0].parent_id in stack_ids[:-1] and b[0].id in b[0].parent_entry.rdeps)\n",
        "            shift_case = 2\n",
        "\n",
        "\n",
        "        if len(scores[SWAP]) == 0 :\n",
        "            swap_cost = 1\n",
        "        elif s0[0].projective_order > b[0].projective_order:\n",
        "            swap_cost = 0\n",
        "            #disable all the others\n",
        "            left_cost = right_cost = shift_cost = 1\n",
        "        else:\n",
        "            swap_cost = 1\n",
        "\n",
        "        costs = (left_cost, right_cost, shift_cost, swap_cost,1)\n",
        "        return costs, shift_case\n",
        "\n",
        "    def oracle_updates(self, best, config, shift_case):\n",
        "        stack_ids = config.get_stack_ids()\n",
        "        if best[1] == SHIFT:\n",
        "            if shift_case == 2:\n",
        "                b = config.get_buffer_head() # Head buffer element\n",
        "                if b[0].parent_entry.id in stack_ids[:-1] and b[0].id in b[0].parent_entry.rdeps:\n",
        "                    b[0].parent_entry.rdeps.remove(b[0].id)\n",
        "                blocked_deps = [d for d in b[0].rdeps if d in stack_ids]\n",
        "                for d in blocked_deps:\n",
        "                    b[0].rdeps.remove(d)\n",
        "\n",
        "        elif best[1] == LEFT_ARC or best[1] == RIGHT_ARC:\n",
        "            s0 = config.get_stack_last_element() # Last stack element\n",
        "            s0[0].rdeps = []\n",
        "            if s0[0].id in s0[0].parent_entry.rdeps:\n",
        "                s0[0].parent_entry.rdeps.remove(s0[0].id)\n",
        "\n",
        "    def error_append(self, info, best, bestValid, bestWrong):\n",
        "        #labeled errors\n",
        "        if best[1] == LEFT_ARC or best[1] == RIGHT_ARC:\n",
        "            s0 = config.get_stack_last_element() # Last stack element\n",
        "            child = s0[0]\n",
        "            if (child.pred_parent_id != child.parent_id or child.pred_relation != child.relation):\n",
        "                info[\"lerrors\"] += 1\n",
        "                #attachment error\n",
        "                if child.pred_parent_id != child.parent_id:\n",
        "                    info[\"eerrors\"] += 1\n",
        "\n",
        "        if bestValid[2] < bestWrong[2] + 1.0:\n",
        "            loss = bestWrong[3] - bestValid[3]\n",
        "            info[\"mloss\"] += 1.0 + bestWrong[2] - bestValid[2]\n",
        "            info[\"eloss\"] += 1.0 + bestWrong[2] - bestValid[2]\n",
        "            info[\"errs\"].append(loss)\n",
        "\n",
        "        #??? when did this happen and why?\n",
        "        if best[1] == 0 or best[1] == 2:\n",
        "            info[\"etotal\"] += 1\n",
        "\n",
        "    def cost_best_wrong_valid(self, scores, costs):\n",
        "        s0 = config.get_stack_last_element() # Last stack element\n",
        "        bestValid = list(( s for s in chain(*scores) if costs[s[1]] == 0 and \\\n",
        "            ( s[1] == SHIFT or s[1] == SWAP or  s[0] == s0[0].relation ) ))\n",
        "\n",
        "        bestValid = max(bestValid, key=itemgetter(2))\n",
        "        bestWrong = max(( s for s in chain(*scores) if costs[s[1]] != 0 or \\\n",
        "            ( s[1] != SHIFT and s[1] != SWAP and s[0] != s0[0].relation ) ), key=itemgetter(2))\n",
        "        #force swap\n",
        "        if costs[SWAP]== 0:\n",
        "            best = bestValid\n",
        "        else:\n",
        "        #select a transition to follow\n",
        "        # + aggresive exploration\n",
        "        #1: might want to experiment with that parameter\n",
        "            if bestWrong[1] == SWAP:\n",
        "                best = bestValid\n",
        "            else:\n",
        "                best = bestValid if ( (not self.oracle) or (bestValid[2] - bestWrong[2] > 1.0) \\\n",
        "                    or (bestValid[2] > bestWrong[2] and random.random() > 0.1) ) else bestWrong\n",
        "        return best, bestValid, bestWrong\n",
        "\n",
        "    def train_sentence(self, sentence, info, irels, embeds):\n",
        "            config = Configuration(sentence, irels)\n",
        "\n",
        "            ninf = -float('inf')\n",
        "            while not config.is_end():\n",
        "                #print(config)\n",
        "                scores = self.__evaluate(config, True, irels, embeds)\n",
        "                scores.append([(None, 4, ninf, None)]) \n",
        "                #to ensure that we have at least one wrong operation # TODO: зачем нужно?\n",
        "\n",
        "                costs, shift_case = self.calculate_cost(config, scores)\n",
        "        \n",
        "                best, bestValid, bestWrong = cost_best_wrong_valid(self, scores, costs)\n",
        "\n",
        "                #updates for the dynamic oracle\n",
        "                if self.oracle: # TODO: проверить, что значит True/False\n",
        "                    self.oracle_updates(best, config, shift_case)\n",
        "\n",
        "                self.error_append(info, best, bestValid, bestWrong) \n",
        "                # TODO: Проверить взаимосвязь с oracle_update. Если можно, перенести error_append в cost_best_wrong_valid\n",
        "\n",
        "                config.apply_transition(best)\n",
        "\n",
        "            return info\n",
        "\n",
        "    def error_processing(self, info):\n",
        "        errs = info[\"errs\"]\n",
        "        self.labeled_optimizer.zero_grad()\n",
        "        self.unlabeled_optimizer.zero_grad()\n",
        "        eerrs = torch.sum(torch.tensor(errs, requires_grad=True))\n",
        "        eerrs.backward()\n",
        "        self.labeled_optimizer.step() # TODO Какой из оптимизаторов ???\n",
        "        self.unlabeled_optimizer.step()\n",
        "        info[\"errs\"] = []\n",
        "\n",
        "    def create_info(self):\n",
        "        info = {}\n",
        "        info[\"mloss\"], info[\"eloss\"], info[\"eerrors\"], info[\"lerrors\"], info[\"etotal\"]  = 0.0, 0.0, 0, 0, 0\n",
        "        info[\"errs\"] = []\n",
        "        info[\"iSentence\"] = -1\n",
        "        info[\"start\"] = time.time()\n",
        "        return info\n",
        "\n",
        "    def train_logging(self, info):\n",
        "        loss_message = (\n",
        "            f'Processing sentence number: {info[\"iSentence\"]}'\n",
        "            f' Loss: {info[\"eloss\"] / info[\"etotal\"]:.3f}'\n",
        "            f' Errors: {info[\"eerrors\"] / info[\"etotal\"]:.3f}'\n",
        "            f' Labeled Errors: {info[\"lerrors\"] / info[\"etotal\"]:.3f}'\n",
        "            f' Time: {time.time()-info[\"start\"]:.3f}s'\n",
        "        )\n",
        "        logging.debug(loss_message)\n",
        "        info[\"start\"] = time.time() # TODO: зачем этот параметр ?\n",
        "        info[\"eerrors\"], info[\"eloss\"], info[\"etotal\"], info[\"lerrors\"] = 0, 0.0, 0, 0 # TODO: Почему здесь зануляем?\n",
        "\n",
        "    def Train(self, trainData, options, irels, embeds):\n",
        "        random.shuffle(trainData) \n",
        "        # in certain cases the data will already have been shuffled after being read from file or while creating dev data\n",
        "        logging.info(f\"Length of training data: {len(trainData)}\")\n",
        "\n",
        "        beg = time.time()\n",
        "        info = self.create_info()\n",
        "\n",
        "        pbar = tqdm.tqdm(\n",
        "            trainData, desc=\"Training\", unit=\"sentences\",\n",
        "            mininterval=1.0, leave=False, disable=False,\n",
        "        )\n",
        "\n",
        "        for iSentence, sentence in enumerate(pbar,1):\n",
        "            #print(\"-----------------------------------------------\")\n",
        "            #print(\"Sentence №\", iSentence, sentence[2])\n",
        "            info[\"iSentence\"] = iSentence\n",
        "            if iSentence % 100 == 0:\n",
        "                self.train_logging(info)\n",
        "\n",
        "            info = self.train_sentence(sentence, info, irels, embeds)\n",
        "\n",
        "            #footnote 8 in Eli's original paper\n",
        "            if len(info[\"errs\"]) > 50: # or True:\n",
        "                self.error_processing(info)\n",
        "\n",
        "        if len(info[\"errs\"]) > 0:\n",
        "            self.error_processing(info)\n",
        "\n",
        "        logging.info(f\"Loss: {info['mloss']/info['iSentence']}\")\n",
        "        logging.info(f\"Total Training Time: {time.time()-beg:.2g}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01G-nT6mB7H"
      },
      "source": [
        "# uuparser/parser.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "484-h9BXlnU7"
      },
      "outputs": [],
      "source": [
        "import pickle, os, time, sys, copy, itertools, re, random\n",
        "\n",
        "from shutil import copyfile\n",
        "\n",
        "def evaluate_uas(sentence_descr):\n",
        "    #sentence_descr is a list, in which elements 0, 1, 2 are auxiliary\n",
        "    right_parent_tokens = 0\n",
        "    for token in sentence_descr[3:]:\n",
        "        if isinstance(token, ConllEntry): # TODO: изучить случаи, когда не ConllEntry - ошибка считывания?\n",
        "          if token.pred_parent_id == token.parent_id:\n",
        "              right_parent_tokens += 1\n",
        "        #print(\"pred_parent:\", token.pred_parent_id, \"real_parent:\", token.parent_id)\n",
        "    uas = right_parent_tokens / (len(sentence_descr) - 3)\n",
        "    return uas\n",
        "\n",
        "def evaluate_uas_epoche(sentence_list):\n",
        "    summ_uas = 0\n",
        "    for sent in sentence_list:\n",
        "        summ_uas += evaluate_uas(sent)\n",
        "    return summ_uas / len(sentence_list)\n",
        "\n",
        "def run(traindata, valdata, testdata, embeds, options):\n",
        "\n",
        "    irels = get_irels(traindata)\n",
        "    out_irels_dims = len(irels)\n",
        "    logging.debug('Initializing the model')\n",
        "    parser = Parser(options, out_irels_dims)\n",
        "\n",
        "    dev_best = [options[\"epochs\"],-1.0] # best epoch, best score\n",
        "\n",
        "    for epoch in range(options[\"first_epoch\"], options[\"epochs\"] + 1):\n",
        "        # Training\n",
        "        logging.info(f'Starting epoch {epoch} (training)')\n",
        "        parser.Train(traindata,options, irels, embeds)\n",
        "        logging.info(f'Finished epoch {epoch} (training)')\n",
        "\n",
        "        parser.Save(epoch)\n",
        "\n",
        "        logging.info(f\"Predicting on dev data\")\n",
        "        dev_pred = list(parser.Predict(valdata,\"dev\",options, irels, embeds))\n",
        "        mean_dev_score = evaluate_uas_epoche(dev_pred)\n",
        "        logging.info(f\"Dev score {mean_dev_score:.2f} at epoch {epoch:d}\")\n",
        "        print(f\"Dev score {mean_dev_score:.2f} at epoch {epoch:d}\")\n",
        "\n",
        "        if mean_dev_score > dev_best[1]:\n",
        "            dev_best = [epoch,mean_dev_score] # update best dev score\n",
        "\n",
        "    logging.info(f\"Loading best model from epoche{dev_best[0]:d}\")\n",
        "    # Loading best_models to parser.labeled_GNN and parser.unlabeled_GNN\n",
        "    parser.Load(epoch)\n",
        "\n",
        "    logging.info(f\"Predicting on test data\")\n",
        "\n",
        "    test_pred = list(parser.Predict(testdata,\"test\",options, irels, embeds))\n",
        "    mean_test_score = evaluate_uas_epoche(test_pred)\n",
        "\n",
        "    logging.info(f\"On test obtained UAS score of {mean_test_score:.2f}\")\n",
        "    print(f\"On test obtained UAS score of {mean_test_score:.2f}\")\n",
        "\n",
        "\n",
        "    logging.debug('Finished predicting')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "5J4o7_QiH6JH"
      },
      "outputs": [],
      "source": [
        "options = {}\n",
        "options[\"hidden_dims\"] = 100 # MLP hidden layer dimensions\n",
        "options[\"learning_rate\"] = 0.001 # Learning rate for neural network optimizer\n",
        "\n",
        "options[\"oracle\"] = True # Use the static oracle instead of the dynamic oracle\n",
        "\n",
        "options[\"epochs\"] = 10 # Number of epochs\n",
        "options[\"first_epoch\"] = 1\n",
        "\n",
        "# really important to do this before anything else to make experiments reproducible\n",
        "set_seeds()\n",
        "\n",
        "train_dir = 'sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu'\n",
        "val_dir = 'sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu'\n",
        "test_dir = 'sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu'\n",
        "\n",
        "train, train_words = read_conll(train_dir)\n",
        "val, val_words = read_conll(val_dir)\n",
        "test, test_words = read_conll(test_dir)\n",
        "all_words = train_words + val_words + test_words\n",
        "all_words = set(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "0fDj1MJpJBV3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "def get_embed(tokenizer, model, word):\n",
        "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.last_hidden_state[0][0]\n",
        "    return last_hidden_states.detach().cpu()\n",
        "\n",
        "embeds = {}\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "logging.debug('Creating embeddings')\n",
        "ts = time.time()\n",
        "for word in all_words:\n",
        "    embeds[word] = get_embed(tokenizer, model, word)\n",
        "logging.debug(f'{len(embeds)} embeddings were created')\n",
        "te = time.time()\n",
        "logging.info(f'Time of embedding creation: {te-ts:.2g}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4dk0TJSIASC",
        "outputId": "15e0d889-e9a4-42c7-9114-1ef399f4ee20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IbKLEjhM2L9",
        "outputId": "4f0eca43-8cb6-436d-f925-0868f8918a41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root - WARNING - This will get logged to a file\n",
            "root - WARNING - New warning\n",
            "root - DEBUG - New debug\n",
            "root - INFO - New info\n",
            "root - DEBUG - Using default Python seed\n",
            "root - INFO - Reading sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu\n",
            "root - DEBUG - 46 sentences read\n",
            "root - INFO - Time: 0.016s\n",
            "root - INFO - Reading sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu\n",
            "root - DEBUG - 25 sentences read\n",
            "root - INFO - Time: 0.0085s\n",
            "root - INFO - Reading sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu\n",
            "root - DEBUG - 27 sentences read\n",
            "root - INFO - Time: 0.25s\n",
            "root - DEBUG - Creating embeddings\n",
            "root - DEBUG - 781 embeddings were created\n",
            "root - INFO - Time of embedding creation: 87s\n",
            "root - DEBUG - Initializing the model\n",
            "root - INFO - Starting epoch 1 (training)\n",
            "root - INFO - Length of training data: 46\n",
            "root - DEBUG - Using default Python seed\n",
            "root - INFO - Reading sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu\n",
            "root - DEBUG - 46 sentences read\n",
            "root - INFO - Time: 0.018s\n",
            "root - INFO - Reading sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu\n",
            "root - DEBUG - 25 sentences read\n",
            "root - INFO - Time: 0.014s\n",
            "root - INFO - Reading sample_data/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu\n",
            "root - DEBUG - 27 sentences read\n",
            "root - INFO - Time: 0.016s\n",
            "root - DEBUG - Creating embeddings\n",
            "root - DEBUG - 781 embeddings were created\n",
            "root - INFO - Time of embedding creation: 85s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(Path('/content/app.log').read_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNypIrvK-8yt",
        "outputId": "cd0c239a-bd4f-40b1-ae62-a62a7b3edca5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.20 at epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.20 at epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.20 at epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.18 at epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.19 at epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.19 at epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.19 at epoch 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.19 at epoch 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.18 at epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev score 0.19 at epoch 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                               "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On test obtained UAS score of 0.07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "run(train, val, test, embeds, options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zsjh8wJiqZw",
        "outputId": "74b03cca-7ef0-451d-de08-9cff7a68e130"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'stack_ids': [],\n",
              " 's1': [],\n",
              " 's0': [],\n",
              " 'b': [<__main__.ConllEntry at 0x7cf5c478f280>],\n",
              " 'beta': [<__main__.ConllEntry at 0x7cf5c478f220>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478ef20>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478ce20>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f3a0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f190>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f730>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f340>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f760>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478e3e0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478d480>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f3d0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f7c0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478ee60>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478dde0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478dc00>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478e800>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478cf70>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478efe0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f0d0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f400>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478d840>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f850>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f7f0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f160>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f820>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478d600>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f790>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f460>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f2e0>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f100>,\n",
              "  <__main__.ConllEntry at 0x7cf5c474c250>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f430>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f370>,\n",
              "  <__main__.ConllEntry at 0x7cf5c478f2b0>],\n",
              " 'costs': (1, 1, 0, 1, 1),\n",
              " 'shift_case': 2}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB8WaJlLkGDT"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0jIrXhiBLRM"
      },
      "source": [
        "TODO:\n",
        "В sentence последний элемент -\n",
        "\n",
        "{'id': 0,\n",
        " 'form': '*root*',\n",
        " 'char_rep': '*root*',\n",
        " 'norm': '*root*',\n",
        " 'cpos': 'ROOT-CPOS',\n",
        " 'pos': 'ROOT-POS',\n",
        " 'parent_id': -1,\n",
        " 'relation': 'rroot',\n",
        " 'lemma': '*root*',\n",
        " 'feats': '_',\n",
        " 'deps': '_',\n",
        " 'misc': '_',\n",
        " 'pred_parent_id': None,\n",
        " 'pred_relation': None,\n",
        " 'treebank_id': None,\n",
        " 'proxy_tbank': None,\n",
        " 'pred_pos': None,\n",
        " 'pred_cpos': None,\n",
        " 'projective_order': 0,\n",
        " 'rdeps': [8],\n",
        " 'children': [],\n",
        " 'scores': None,\n",
        " 'parent': None,\n",
        " 'vecs': None}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1apfPV9kHRa"
      },
      "source": [
        "В какую сторону стек в коде сейчас ?\n",
        "Используют stack[-1], stack[-2].\n",
        "Стек или очередь ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su65X20Amqjk"
      },
      "source": [
        "Разобраться, какие метрики считают при обучении (на train)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2yL_DsoclYTr",
        "0jMvUaVzlnEt",
        "FeMXyAEjgagf"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
